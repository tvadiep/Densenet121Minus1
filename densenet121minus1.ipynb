{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from __future__ import division\nfrom __future__ import print_function\n\nimport os\nimport sys\nimport json\nimport pprint\nimport warnings\nimport pandas as pd\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.utils.data as data\nimport torch.nn.utils.prune as prune\nfrom torch.optim import lr_scheduler\nimport torchvision.transforms as transforms\n\nimport sklearn.metrics\nfrom collections import OrderedDict\n\nfrom torch.autograd import Variable\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2021-11-13T01:08:10.122456Z","iopub.execute_input":"2021-11-13T01:08:10.123002Z","iopub.status.idle":"2021-11-13T01:08:12.338314Z","shell.execute_reply.started":"2021-11-13T01:08:10.122967Z","shell.execute_reply":"2021-11-13T01:08:12.337552Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Data Retriver","metadata":{}},{"cell_type":"code","source":"class Parser():\n  def __init__(self, model = \"densenet\", optimizer = \"adam\", lr = 0.0001, weight_decay = 0.0, drop_rate = 0.0, epochs = 165,\\\n               batch_size = 16, workers = 8, seed = 123456, tag = \"\", toy = False, save_path =\"./\", scale = 224, horizontal_flip = True,\\\n               verbose = True, scratch = True, train_weighted = True, valid_weighted = True, size = None):\n    self.model = model; self.optimizer = optimizer; self.lr = lr; self.weight_decay = weight_decay; self.drop_rate = drop_rate\n    self.epochs = epochs; self.batch_size = batch_size; self.workers = workers; self.seed = seed; self.tag = tag; self.toy = toy\n    self.save_path = save_path; self.scale = scale; self.horizontal_flip = horizontal_flip; self.verbose = verbose; self.scratch = scratch\n    self.train_weighted = train_weighted; self.valid_weighted = valid_weighted; self.size = size\n\nclass Dataset(data.Dataset):\n    def __init__(self, args, data_split):\n        super(Dataset, self).__init__()\n        tag = \"\"\n        if args.tag:\n          tag = \"_\"+args.tag\n        df = pd.read_csv(os.path.join(\"../input/latestdata/%s%s.csv\" % (data_split, tag)))\n        df['Path'] = df['Image Index']\n        \n        # Có thử nghiệm hay không? frac = tỉ lệ data lấy ra từ dataset\n        if args.toy:\n            df = df.sample(frac=0.01)\n\n        self.df = df\n        self.img_paths = df[\"Path\"].tolist()\n        self.pathologies = [col for col in df.columns.values if (col != \"Path\" and col != \"No Finding\" and col != 'Image Index')]\n        self.labels = df[self.pathologies].to_numpy().astype(int)\n        \n        self.n_classes = self.labels.shape[1]\n\n        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                         std=[0.229, 0.224, 0.225])\n        if data_split == \"train\":\n            transforms_lst = [\n                transforms.Resize((args.scale, args.scale)),\n                transforms.RandomHorizontalFlip() if args.horizontal_flip else None,\n                transforms.ToTensor(),\n                normalize,\n            ]\n            self.transform = transforms.Compose([t for t in transforms_lst if t])\n        else:\n            self.transform = transforms.Compose([\n                transforms.Resize((args.scale, args.scale)),\n                transforms.ToTensor(),\n                normalize,\n            ])\n        self.df = df\n                \n        if (data_split == \"train\" and args.train_weighted) or (data_split == \"valid\" and args.valid_weighted):\n            self.get_weights(args, data_split)\n\n\n    def get_weights(self, args, data_split):\n\n        self.use_gpu = torch.cuda.is_available()\n        p_count = (self.labels == 1).sum(axis = 0)\n        self.p_count = p_count\n        n_count = (self.labels == 0).sum(axis = 0)\n        total = p_count + n_count\n\n        # invert *opposite* weights to obtain weighted loss\n        # (positives weighted higher, all weights same across batches, and p_weight + n_weight == 1)\n        p_weight = n_count / total\n        n_weight = p_count / total\n\n        self.p_weight_loss = Variable(torch.FloatTensor(p_weight), requires_grad=False)\n        self.n_weight_loss = Variable(torch.FloatTensor(n_weight), requires_grad=False)\n\n        print (\"Positive %s Loss weight:\" % data_split, self.p_weight_loss.data.numpy())\n        print (\"Negative %s Loss weight:\" % data_split, self.n_weight_loss.data.numpy())\n        random_loss = sum((p_weight[i] * p_count[i] + n_weight[i] * n_count[i]) *\\\n                                               -np.log(0.5) / total[i] for i in range(self.n_classes)) / self.n_classes\n        print (\"Random %s Loss:\" % data_split, random_loss)\n\n\n    def __getitem__(self, index):\n        img = Image.open(self.img_paths[index]).convert(\"RGB\")\n        label = self.labels[index]\n\n        return self.transform(img), torch.LongTensor(label)\n\n    def __len__(self):\n        return len(self.img_paths)\n\n    def weighted_loss(self, preds, target, epoch=1):\n\n        weights = target.type(torch.FloatTensor) * (self.p_weight_loss.expand_as(target)) + \\\n                  (target == 0).type(torch.FloatTensor) * (self.n_weight_loss.expand_as(target))\n        if self.use_gpu:\n            weights = weights.cuda()\n        loss = 0.0\n        for i in range(self.n_classes):\n            loss += nn.functional.binary_cross_entropy_with_logits(preds[:,i], target[:,i], weight=weights[:,i])\n        return loss / self.n_classes\n\n\ndef evaluate(gts, probabilities, pathologies, use_only_index = None):\n    assert(np.all(probabilities >= 0) == True)\n    assert(np.all(probabilities <= 1) == True)\n\n    def compute_metrics_for_class(i):\n         p, r, t = sklearn.metrics.precision_recall_curve(gts[:, i], probabilities[:, i])\n         PR_AUC = sklearn.metrics.auc(r, p)\n         ROC_AUC = sklearn.metrics.roc_auc_score(gts[:, i], probabilities[:, i])\n         F1 = sklearn.metrics.f1_score(gts[:, i], preds[:, i])\n         acc = sklearn.metrics.accuracy_score(gts[:, i], preds[:, i])\n         count = np.sum(gts[:, i])\n         return PR_AUC, ROC_AUC, F1, acc, count\n\n    PR_AUCs = []\n    ROC_AUCs = []\n    F1s = []\n    accs = []\n    counts = []\n    preds = probabilities >= 0.5\n\n    classes = [use_only_index] if use_only_index is not None else range(len(gts[0]))\n\n    for i in classes:\n        try:\n            PR_AUC, ROC_AUC, F1, acc, count = compute_metrics_for_class(i)\n        except ValueError:\n            continue\n        PR_AUCs.append(PR_AUC)\n        ROC_AUCs.append(ROC_AUC)\n        F1s.append(F1)\n        accs.append(acc)\n        counts.append(count)\n        print('Class: {!s} Count: {:d} PR AUC: {:.4f} ROC AUC: {:.4f} F1: {:.3f} Acc: {:.3f}'.format(pathologies[i], count, PR_AUC, ROC_AUC, F1, acc))\n\n    avg_PR_AUC = np.average(PR_AUCs)\n    avg_ROC_AUC = np.average(ROC_AUCs, weights=counts)\n    avg_F1 = np.average(F1s, weights=counts)\n\n    print('Avg PR AUC: {:.3f}'.format(avg_PR_AUC))\n    print('Avg ROC AUC: {:.3f}'.format(avg_ROC_AUC))\n    print('Avg F1: {:.3f}'.format(avg_F1))\n    return avg_PR_AUC, avg_ROC_AUC, avg_F1\n\n\ndef loader_to_gts(data_loader):\n    gts = []\n    for (inputs, labels) in data_loader:\n        for label in labels.cpu().numpy().tolist():\n            gts.append(label)\n    gts = np.array(gts)\n    return gts\n\n\ndef load_data(args):\n\n    train_dataset = Dataset(args, \"train\")\n    valid_dataset = Dataset(args, \"valid\")\n\n    train_loader = torch.utils.data.DataLoader(\n            train_dataset, batch_size=args.batch_size, shuffle=True,\n            num_workers=args.workers, pin_memory=True, sampler=None)\n    print(\"train shape la: \",train_dataset.__len__())\n    print(\"valid shape la: \", valid_dataset.__len__())\n\n    valid_loader = torch.utils.data.DataLoader(\n            valid_dataset, batch_size=args.batch_size, shuffle=False,\n            num_workers=args.workers, pin_memory=True, sampler=None)\n\n    return train_loader, valid_loader\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T01:08:12.340170Z","iopub.execute_input":"2021-11-13T01:08:12.340427Z","iopub.status.idle":"2021-11-13T01:08:12.378655Z","shell.execute_reply.started":"2021-11-13T01:08:12.340393Z","shell.execute_reply":"2021-11-13T01:08:12.377850Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# Densenet configuration","metadata":{}},{"cell_type":"code","source":"# Define các hằng số\n\nn_patho = 14\nmodel_path = {\n    'densenet121': '../input/densenet121/densenet121.pth'\n}\n\n# Tùy chỉnh Densenet\nclass _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm-1', nn.BatchNorm2d(num_input_features)),\n        self.add_module('relu-1', nn.ReLU(inplace=True)),\n        self.add_module('conv-1', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module('norm-2', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module('relu-2', nn.ReLU(inplace=True)),\n        self.add_module('conv-2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n\n\nclass _DenseBlock(nn.Sequential):\n    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n        super(_DenseBlock, self).__init__()\n        for i in range(num_layers):\n            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n            self.add_module('denselayer%d' % (i + 1), layer)\n\n\nclass _Transition(nn.Sequential):\n    def __init__(self, num_input_features, num_output_features):\n        super(_Transition, self).__init__()\n        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n        self.add_module('relu', nn.ReLU(inplace=True))\n        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n                                          kernel_size=1, stride=1, bias=False))\n        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n\n\nclass DenseNet(nn.Module):\n    r\"\"\"Densenet-BC model class, based on\n    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    \"\"\"\n    def __init__(self, num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n                 bn_size=4, drop_rate=0, num_classes=1000):\n\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            ('norm0', nn.BatchNorm2d(num_init_features)),\n            ('relu0', nn.ReLU(inplace=True)),\n            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module('denseblock%d' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n                self.features.add_module('transition%d' % (i + 1), trans)\n                num_features = num_features // 2\n\n        # Final batch norm\n        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n#        features = self.features(x)\n        f = self.features.conv0(x)\n        f = self.features.norm0(f)\n        f = self.features.relu0(f)\n        f = self.features.pool0(f)\n        \n        # 3 blocks and 2 transitions\n        f = self.features.denseblock1(f)\n        f = self.features.transition1(f)\n        f = self.features.denseblock2(f)\n        f = self.features.transition2(f)\n        f = self.features.denseblock3(f)\n        \n        out = F.relu(f, inplace=True)\n        out = self.gap(out).view(f.size(0), -1)\n        out = self.classifier(out)\n        return out\n    \n# def densenet121(pretrained=True, **kwargs):\n#     r\"\"\"Densenet-121 model from\n#     `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_\n#     Args:\n#         pretrained (bool): If True, returns a model pre-trained on ImageNet\n#     \"\"\"\n#     model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16), bn_size=4, drop_rate = 0, num_classes=1000)\n#     if pretrained:\n#         model.load_state_dict(torch.load(model_path['densenet121']), strict=False)\n#     return model\n\ndef densenet121(pretrained = True):\n    if(pretrained == True):\n        model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\n    else:\n        model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=False)\n    def __init__(self):\n        super().__init__()\n        self.net = model\n\n        \n        # Final batch norm\n        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n\n        self.gap = nn.AdaptiveAvgPool2d(1)\n\n        # Linear layer\n        self.pooling = nn.AvgPool2d()\n        self.classifier = nn.Linear(32,14)\n    def forward(self, x):\n        f = self.net.features.conv0(x)\n        f = self.net.features.norm0(f)\n        f = self.net.features.relu0(f)\n        f = self.net.features.pool0(f)\n        \n        # 3 blocks and 2 transitions\n        f = self.net.features.denseblock1(f)\n        f = self.net.features.transition1(f)\n        f = self.net.features.denseblock2(f)\n        f = self.net.features.transition2(f)\n        f = self.net.features.denseblock3(f)\n        \n        # classification block containing a global average pooling layer followed by a fully connected layer \n        self.pooling(f)\n        self.classifier(f)\n        return out","metadata":{"execution":{"iopub.status.busy":"2021-11-13T01:08:12.380031Z","iopub.execute_input":"2021-11-13T01:08:12.380293Z","iopub.status.idle":"2021-11-13T01:08:12.411128Z","shell.execute_reply.started":"2021-11-13T01:08:12.380260Z","shell.execute_reply":"2021-11-13T01:08:12.410459Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"model = torch.hub.load('pytorch/vision:v0.10.0', 'densenet121', pretrained=True)\nmodel.features","metadata":{"execution":{"iopub.status.busy":"2021-11-13T01:08:12.412972Z","iopub.execute_input":"2021-11-13T01:08:12.413403Z","iopub.status.idle":"2021-11-13T01:08:21.325548Z","shell.execute_reply.started":"2021-11-13T01:08:12.413367Z","shell.execute_reply":"2021-11-13T01:08:21.324762Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"\"\"\"\nTrain script for CheXNet\n\"\"\"\n\ndef transform_data(data, use_gpu, train=False):\n    inputs, labels = data\n    labels = labels.type(torch.FloatTensor)\n    if use_gpu is True:\n        inputs = inputs.cuda()\n        labels = labels.cuda()\n    inputs = Variable(inputs, requires_grad=False, volatile=not train)\n    labels = Variable(labels, requires_grad=False, volatile=not train)\n    return inputs, labels\n\n\ndef train_epoch(epoch, args, model, loader, criterion, optimizer):\n    model.train()\n    batch_losses = []\n    for batch_idx, data in enumerate(loader):\n        inputs, labels = transform_data(data, True, train=True)\n        optimizer.zero_grad()\n        outputs = model(inputs)\n        print('shape labels: ', labels.shape)\n        print('shape output: ', outputs.shape)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        print(\"Epoch: {:d} Batch: {:d} ({:d}) Train Loss: {:.6f}\".format(\n            epoch, batch_idx, args.batch_size, loss.data))\n        sys.stdout.flush()\n        batch_losses.append(loss.data)\n    #train_loss = np.mean(batch_losses)\n    train_loss = torch.stack(batch_losses).mean().item()\n    print(\"Training Loss: {:.6f}\".format(train_loss))\n    return train_loss\n\n\ndef test_epoch(model, loader, criterion, epoch=1):\n    \"\"\"\n    Returns: (AUC, ROC AUC, F1, validation loss)\n    \"\"\"\n    model.eval()\n    test_losses = []\n    outs = []\n    gts = []\n    for data in loader:\n        for gt in data[1].numpy().tolist():\n            gts.append(gt)\n        inputs, labels = transform_data(data, True, train=False)\n        outputs = model(inputs)\n        loss = criterion(outputs, labels, epoch=epoch)\n        test_losses.append(loss.data)\n        out = torch.sigmoid(outputs).data.cpu().numpy()\n        outs.extend(out)\n    #avg_loss = np.mean(test_losses)\n    avg_loss = torch.stack(test_losses).mean().item()\n    print(\"Validation Loss: {:.6f}\".format(avg_loss))\n    outs = np.array(outs)\n    gts = np.array(gts)\n    return evaluate(gts, outs, loader.dataset.pathologies) + (avg_loss,)\n\n\ndef get_loss(dataset, weighted):\n\n    criterion = nn.MultiLabelSoftMarginLoss()\n\n    def loss(preds, target, epoch):\n\n        if weighted:\n\n            return dataset.weighted_loss(preds, target, epoch=epoch)\n\n        else:\n\n            return criterion(preds, target)\n\n    return loss\n\n\ndef run(args):\n\n    use_gpu = torch.cuda.is_available()\n    model = densenet121(pretrained=True, num_classes = 14)\n\n    train, val = load_data(args)\n    print(\"train.shape = {}\",len(train))\n    print(\"val.shape = {}\", len(val))\n    nclasses = train.dataset.n_classes\n    print(\"Number of classes:\", nclasses)\n    \n    if args.model == \"densenet\":\n        model = densenet121(pretrained=True, num_classes = nclasses)\n    else:\n        print(\"{} is not a valid model.\".format(args.model))\n    \n    if use_gpu:\n        model = model.cuda()\n\n    #train_criterion = get_loss(train.dataset, args.train_weighted)\n    train_criterion = F.binary_cross_entropy\n    val_criterion = get_loss(val.dataset, args.valid_weighted)\n\n    if args.optimizer == \"adam\":\n        optimizer = optim.Adam(\n                       filter(lambda p: p.requires_grad, model.parameters()),\n                       lr=args.lr,\n                       weight_decay=args.weight_decay)\n    elif args.optimizer == \"rmsprop\":\n        optimizer = optim.RMSprop(\n                       filter(lambda p: p.requires_grad, model.parameters()),\n                       lr=args.lr,\n                       weight_decay=args.weight_decay)\n    else:\n        print(\"{} is not a valid optimizer.\".format(args.optimizer))\n\n    scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, threshold=0.001, factor=0.1)\n    best_model_wts, best_loss = model.state_dict(), float(\"inf\")\n\n    counter = 0\n    for epoch in range(1, args.epochs + 1):\n        print(\"Epoch {}/{}\".format(epoch, args.epochs))\n        print(\"-\" * 10)\n        train_loss = train_epoch(epoch, args, model, train,train_criterion, optimizer)\n        _, epoch_auc, _, valid_loss = test_epoch(model, val, val_criterion, epoch)\n        scheduler.step(valid_loss)\n\n        if (valid_loss < best_loss):\n            best_loss = valid_loss\n            best_model_wts = model.state_dict()\n            counter = 0\n        else:\n            counter += 1\n\n        if counter > 3:\n            print(\"ran out of patience....\")\n            break\n\n        torch.save(best_model_wts, os.path.join(args.save_path, \"val%f_train%f_epoch%d\" % (valid_loss, train_loss, epoch)))\n\n    print(\"Best Validation Loss:\", best_loss)\n\nif __name__ == \"__main__\":\n    \"\"\"\n    Usage\n        Download the images data at https://nihcc.app.box.com/v/ChestXray-NIHCC\n        To train on the original labels:\n            python train.py --save_path run_dir --model densenet --batch_size 8 --horizontal_flip --epochs 10 --lr 0.0001 --train_weighted --valid_weighted --scale 512\n        To train on the relabels:\n            python train.py --save_path run_dir --model densenet --batch_size 8 --horizontal_flip --epochs 10 --lr 0.0001 --train_weighted --valid_weighted --scale 512 --tag relabeled\n    \"\"\"\n    p = Parser()\n    run(p)\n\n","metadata":{"execution":{"iopub.status.busy":"2021-11-13T01:08:21.327142Z","iopub.execute_input":"2021-11-13T01:08:21.327398Z","iopub.status.idle":"2021-11-13T01:08:21.483396Z","shell.execute_reply.started":"2021-11-13T01:08:21.327364Z","shell.execute_reply":"2021-11-13T01:08:21.482526Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}